---
layout: page-sidebar-leaderboard
title: Leaderboard of MSCeleb1M @ ACM Multimedia 2016
permalink: /leaderboard/2016
---

<div role="tabpanel" class="tab-pane active" id="latest">
    <h2>Result of MSCeleb1M @ MM2016</h2>
    <h3>Last Updated: 6/21/2016</h3>
    <ul class="nav nav-tabs" role="tablist">
        <li role="presentation" class="active"><a href="#random" aria-controls="random" role="tab" data-toggle="tab">Random Set</a></li>
        <li role="presentation"><a href="#hard" aria-controls="hard" role="tab" data-toggle="tab">Hard Set</a></li>
        <li role="presentation"><a href="#team" aria-controls="team" role="tab" data-toggle="tab">Team Information</a></li>
    </ul>
    <div class="tab-content">
        <div role="tabpanel" class="tab-pane fade" id="random">
            <table class="table table-bordered table-striped table-hover">
                <tr>
                    <th>Rank</th><th>TeamID</th><th>TeamName</th><th>Data</th><th>Coverage@P=95</th><th>Coverage@P=99</th>
                </tr>
                <tr><td>1</td><td>14</td><td>DRNfLSCR</td><td>Aligned</td><td>0.734</td><td>0.525</td></tr>
                <tr><td>2</td><td>8</td><td>ITRC-SARI</td><td>Aligned</td><td>0.707</td><td>0.108</td></tr>
                <tr><td>3</td><td>13</td><td>CIGIT_NLPR</td><td>Aligned</td><td>0.684</td><td>0.045</td></tr>
                <tr><td>4</td><td>20</td><td>ms3rz</td><td>Aligned</td><td>0.646</td><td>0.343</td></tr>
                <tr><td>5</td><td>10</td><td>1510</td><td>Aligned</td><td>0.57</td><td>0.065</td></tr>
                <tr><td>6</td><td>7</td><td>FaceAll</td><td>Aligned</td><td>0.554</td><td>0.417</td></tr>
                <tr><td>7</td><td>17</td><td>faceman</td><td>Aligned</td><td>0.461</td><td>0.339</td></tr>
                <tr><td>8</td><td>18</td><td>BUPT_PRIS</td><td>Aligned</td><td>0.421</td><td>0.216</td></tr>
                <tr><td>9</td><td>21</td><td>IMMRSB3RZ</td><td>Aligned</td><td>0.171</td><td>0.104</td></tr>
                <tr><td>10</td><td>12</td><td>CIIDIP</td><td>Aligned</td><td>0.154</td><td>0.025</td></tr>
                <tr><td>11</td><td>11</td><td>BUPT_MCPRL</td><td>Cropped</td><td>0.064</td><td>0.006</td></tr>
                <tr><td>12</td><td>19</td><td>NII-UIT-KAORI</td><td>Aligned</td><td>0.001</td><td>0.001</td></tr>
                <tr><td>13</td><td>6</td><td>DS_NFS</td><td>Aligned</td><td>-</td><td>-</td></tr>
                <tr><td>14</td><td>23</td><td>Paparazzi</td><td>Aligned</td><td>-</td><td>-</td></tr>
            </table>
            <h3>Remarks:</h3>
            <ul>
                <li>For the teams processed both Aligned and Cropped faces, we use the best result of the two.</li>
                <li>Coverage = "-" means the precision does not reach 95%/99%, so the coverage number is undefined.</li>
                <li>See team info in the third tab.</li>
            </ul>
        </div>
        <div role="tabpanel" class="tab-pane fade active in" id="hard">
            <table class="table table-bordered table-striped table-hover">
                <tr>
                    <th>Rank</th><th>TeamID</th><th>TeamName</th><th>Data</th><th>Coverage@P=95</th><th>Coverage@P=99</th>
                </tr>
                <tr><td>1</td><td>13</td><td>CIGIT_NLPR</td><td>Aligned</td><td>0.534</td><td>0.026</td></tr>
                <tr><td>2</td><td>14</td><td>DRNfLSCR</td><td>Aligned</td><td>0.486</td><td>0.336</td></tr>
                <tr><td>3</td><td>17</td><td>faceman</td><td>Aligned</td><td>0.330</td><td>0.211</td></tr>
                <tr><td>4</td><td>20</td><td>ms3rz</td><td>Aligned</td><td>0.260</td><td>0.110</td></tr>
                <tr><td>5</td><td>7</td><td>FaceAll</td><td>Aligned</td><td>0.254</td><td>0.142</td></tr>
                <tr><td>6</td><td>18</td><td>BUPT_PRIS</td><td>Aligned</td><td>0.210</td><td>0.117</td></tr>
                <tr><td>7</td><td>21</td><td>IMMRSB3RZ</td><td>Aligned</td><td>0.042</td><td>0.039</td></tr>
                <tr><td>8</td><td>11</td><td>BUPT_MCPRL</td><td>Cropped</td><td>0.040</td><td>0.007</td></tr>
                <tr><td>9</td><td>12</td><td>CIIDIP</td><td>Aligned</td><td>0.020</td><td>0.018</td></tr>
                <tr><td>10</td><td>8</td><td>ITRC-SARI</td><td>Aligned</td><td>0.004</td><td>0.004</td></tr>
                <tr><td>11</td><td>6</td><td>DS_NFS</td><td>Aligned</td><td>0.001</td><td>0.001</td></tr>
                <tr><td>12</td><td>10</td><td>1510</td><td>Aligned</td><td>0.001</td><td>0.001</td></tr>
                <tr><td>13</td><td>23</td><td>Paparazzi</td><td>Aligned</td><td>-</td><td>-</td></tr>
                <tr><td>14</td><td>19</td><td>NII-UIT-KAORI</td><td>Aligned</td><td>-</td><td>-</td></tr>
            </table>
            <h3>Remarks:</h3>
            <ul>
                <li>For the teams processed both Aligned and Cropped faces, we use the best result of the two.</li>
                <li>Coverage = "-" means the precision does not reach 95%/99%, so the coverage number is undefined.</li>
                <li>See team info in the third tab.</li>
            </ul>
        </div>
        <div role="tabpanel" class="tab-pane fade" id="team">
            <table class="table table-bordered table-striped table-hover">
                        <tr><th>TeamID</th><th>TeamName</th><th>Summary</th><th>Algorithm</th><th>Affiliation</th><th>Team Member</th>                        </tr>
                        <tr><td>5</td><td>HFUT_LMC</td><td></td><td></td><td>Hefei University of Technology</td><td>kecai wu<br>mingyu liou</td></tr>
                        <tr><td>6</td><td>DS_NFS</td><td></td><td></td><td>Institute of Software, Chinese Academy of Science</td><td>Chuan Ke<br>Zi Li<br>Rongrong Xiang<br>Wenbo Li</td></tr>
                        <tr><td>7</td><td>FaceAll</td><td>Face Recogniton with A Simplified CNN and A Two-Stage Training Strategy</td><td>We applied a simplified Googlenet-liked CNN with only 0.6B FLOPS and 4.8M parameters(ignoring the last fully-connected layer) for this evaluation. A two-stage training strategy was used. Firstly, 27,157 classes each with over 100 images(a total of 3,083,130 images) were choosed to train the network in the first stage. Secondly, we sampled 50 images from those classes with over 50 images(due to time limitation) and retained all the images of other classes(a total of 4,791,987 images of 99,891 classes). Only the last fully-connnected layer was finetuned. All the experiments were done in the last week.</td><td>Beijing Faceall Technology Co., Ltd</td><td>Fengye Xiong<br>Zhipeng Yu<br>Yu Guo<br>Hongliang Bai<br>Yuan Dong, Beijing University of Posts and Telecommunications</td></tr>
                        <tr><td>8</td><td>ITRC-SARI</td><td>Face Recognition via Active Image Annotation and Learning</td><td>we introduce an active annotation and learning framework for the face recognition task. Starting with an initial label deficient face image training set, we iteratively training a deep neural network and using this model for sampling the examples for further manual annotation. We follow the active learning strategy and derive an Value of Information criterion to actively select candidate annotation images. During these iterations, the deep neural network is incrementally updated. Experimental results conducted on LFW benchmark and MS-Celeb-1M Challenge demonstrate the effectiveness of our proposed framework.</td><td>Shanghai Advanced Research Institute, Chinese Academy of Sciences</td><td>Hao Ye<br>Weiyuan Shao<br>Hong Wang<br>Yingbin Zheng</td></tr>
                        <tr><td>9</td><td>FutureWorld</td><td></td><td></td><td>Huazhong University of Science and Thechology</td><td>Zhang Chuan</td></tr>
                        <tr><td>10</td><td>1510</td><td>Learning an identity distinguishable space for large scale face recognition</td><td>We firstly use a deep convolutional neural network (CNN) to optimize a 128-bytes embedding for large-scale face retrieval. The embedding is trained via using triplets of aligned face patches from FaceScrub and CASIA-WebFace datasets. Secondly, we leverage the evaluation of MSR Image Recognition according to a cross-domain retrieval scheme. To achieve real-time retrieval, we perform the k-means clustering on the feature space of training data. Furthermore, in order to learn a better similarity measure, we apply a large-scale similary learning on the relevant face images in every cluster. Compared with a lot of existing networks of face recognition, our model is lightweight and our retrieval method is also promising.</td><td>Beijing University of Posts and Telecommunications;</td><td>Jie Shao<br>Zhicheng Zhao<br>Fei Su<br>Zhu Meng<br>Ting Yue</td></tr>
                        <tr><td>11</td><td>BUPT_MCPRL</td><td>Not Disclosed</td><td></td><td></td><td></td></tr>
                        <tr><td>12</td><td>CIIDIP</td><td></td><td></td><td>Tsinghua University</td><td>Yao Zhuliang<br>Chen Dangdang<br>Wang Zhongdao<br>Ge Yunxiang<br>Zhao Yali</td></tr>
                        <tr><td>13</td><td>CIGIT_NLPR</td><td>Weakly Supervised Learning for Web-Scale Face Recognition</td><td>We propose a weakly supervised learning framework for web-scale face recognition. In this framework, a novel constrained pairwise ranking loss is effectively utilized to help alleviate the adverse influence from noise data. We also design an online algorithm to select hard negative image triplets from weakly labeled datasets for model training. Experimental results on MS-Celeb-1M dataset show the effectiveness of our method.</td><td>Chongqing Institute of Green and Intelligent Technology and Institute of Automation, Chinese Academy of Sciences</td><td>Cheng Cheng<br>Junliang Xing<br>Youji Feng<br>Pengcheng Liu<br>Xiao-Hu Shao<br>Xiang-Dong Zhou</td></tr>
                        <tr><td>14</td><td>DRNfLSCR</td><td>A possible solution for large scale classification problem.</td><td>The celebrity recognition is treated as a classification problem. Our approach is based on the Deep Residual Network. All the models used in our system are the 18-layer model. The reason we choose this small model is we don't have enough GPUs and time to use larger models which obviously will get better performance. The data is first randomly split into training set and validation set. The training set contains 90% of all data and the validation set has the rest 10%. We utilize two models to generate the final result. The first model gives 71% and 49.6% coverage@P=95 on the development random set and hard set, respectively. The second model gives about 56.6% and 36.6% coverage@P=95 on the development random set and hard set, respectively. Usually, multi-scale testing gives better results. Considering the speed, we use two-crop testing which only takes the center crop of the original and flipped image. The final output is a tricky fusion of the two models. Our final submission achieves 76.2% and 52.4% coverage@P=95 on the development random set and hard set, respectively.</td><td>Northeast University, USA</td><td>Yue Wu<br>YUN FU</td></tr>
                        <tr><td>15</td><td>UESTC_BMI</td><td></td><td></td><td>UESTC, KB541.</td><td>Feng Wang</td></tr>
                        <tr><td>16</td><td>ZZKDY</td><td></td><td></td><td>ShanghaiTech</td><td>Xu Tang</td></tr>
                        <tr><td>17</td><td>faceman</td><td>In this challenge, we use a two-stage approach for the face identification task: data cleaning and multi-view deep model learning.</td><td>Our approach has two stages. The first stage is data cleaning due to the noisy data in the training set. We first train a ResNet-50 model on the Webface dataset using a classification loss. The activations from the penultimate layer are extracted as the features of images from the MsCeleb dataset. For each person, we apply an outlier detection to remove the noisy data. Specifically, with the feature vector for each image in one class, we calculate the centre of the feature vectors and the Euclidean distance of each feature vector to the centre. Based on the distances, a fix proportion of images in each class are regarded as outliers and are excluded from the training set. The second stage is multi-view deep model learning. Due to the diversity of MsCeleb, we use three deep models which have different structures and loss functions, i.e., ResNet-18, ResNet-50 and GoogLeNet. The ResNet-50 and GoogLeNet are trained with a logistic regression loss, and the ResNet-18 is trained with a triplet loss. These models provides distinct features to better characterize the data distribution from different “views”. Then a 2-layer neural network, whose input is the concatenated features from the penultimate layers of the three deep models, is used to perform multi-view feature fusion and classification. Prediction results with top-5 highest probability is regarded as the final identification result.</td><td>NUS</td><td>Jianshu Li<br>Hao Liu<br>Jian Zhao<br>Fang Zhao</td></tr>
                        <tr><td>18</td><td>BUPT_PRIS</td><td>We train a Lightened CNN network supervised by Joint identification-verification signals and propose a robust object function to deal with this challenge.</td><td>The task of Recognizing One Million Celebrities in the Real World is not like traditional task, in which case there are a large set of training data and a large set of identities. To save training time and disk memory, we use a Lightened CNN network and the Joint identification-verification supervisory signals are used throughout the training stage. As known, there are some noise images and this can decrease the recognition capacity of our deep model. So we modify the object function to handle noise data. Then we use the 8M aligned images of 100k identities and train a 100k-way softmax classifier.</td><td>Beijing University of Posts and Telecommunications</td><td>Binghui Chen,<br>Zhiwen Liu,<br>Mengzi Luo</td></tr>
                        <tr><td>19</td><td>NII-UIT-KAORI</td><td></td><td></td><td>Video Processing Lab - National Institute of Informatics, Japan<br>Multimedia Communications Lab - University of Information Technology, VNU-HCM, Vietnam</td><td>Duy-Dinh Le, National Institute of Informatics; Benjamin Renoust, National Institute of Informatics;<br>Vinh-Tiep Nguyen, University of Information Technology, VNU-HCM;<br>Tien Do, University of Information Technology, VNU-HCM;<br>Thanh Duc Ngo, University of Information Technology, VNU-HCM;<br>Shin'ichi Satoh, National Institute of Informatics;<br>Duc Anh Duong, University of Information Technology, VNU-HCM</td></tr>
                        <tr><td>20</td><td>ms3rz</td><td></td><td></td><td>Institute of Software, Chinese Academy of Sciences;</td><td></td></tr>
                        <tr><td>21</td><td>IMMRSB3RZ</td><td></td><td></td><td>Institute of Software, Chinese Academy of Sciences;</td><td>liu ji<br>Lingyong Yan</td></tr>
                        <tr><td>22</td><td>faceless</td><td></td><td></td><td>CQU</td><td>chen guan hao</td></tr>
                        <tr><td>23</td><td>Paparazzi</td><td>Not Disclosed</td><td></td><td></td><td></td></tr>
                </table>
                <h3>Remarks:</h3>
                <ul>
                    <li>Empty cell means these information are requested to be undisclosed by the participants;</li>
                    <li>Team affiliation and member list are provided by the participants. IRC organizing team doesn't verify them.</li>
                </ul>
        </div>
    </div>
</div>
<div role="tabpanel" class="tab-pane" id="icme16">
    <h1>MSR Image Recognition Challenge (IRC) @ IEEE ICME 2016</h1>
</div>
<div role="tabpanel" class="tab-pane" id="mm15">
    <h1>MSR Image Recognition Challenge (IRC) @ ACM Multimedia 2015</h1>
</div>
<div role="tabpanel" class="tab-pane" id="icme15">
    <h1>MSR Image Recognition Challenge (IRC) @ IEEE ICME 2015</h1>
</div>